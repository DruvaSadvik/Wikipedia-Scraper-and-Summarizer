{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from textstat import flesch_reading_ease\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textblob import TextBlob\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequesties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc = Document('Alexander.docx')\n",
    "summarized_doc = Document('Alexander_summerized.docx')\n",
    "\n",
    "full_text_content = []\n",
    "summarized_text_content = []\n",
    "for paragraph in full_doc.paragraphs:\n",
    "    full_text_content.append(paragraph.text)\n",
    "\n",
    "full_doc_text = '\\n'.join(full_text_content)\n",
    "\n",
    "for paragraph in summarized_doc.paragraphs:\n",
    "    summarized_text_content.append(paragraph.text)\n",
    "\n",
    "summarized_doc_text = '\\n'.join(summarized_text_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retention\n",
    "Evaluate how much of the original content is retained in the summary. This can be measured by comparing the number of words or sentences in the summary to the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Retention: 43.22%\n"
     ]
    }
   ],
   "source": [
    "def calculate_information_retention(original_text, summary):\n",
    "    original_words = set(original_text.lower().split())\n",
    "    summary_words = set(summary.lower().split())\n",
    "    retention = len(summary_words.intersection(original_words)) / len(original_words)\n",
    "    return retention\n",
    "\n",
    "\n",
    "information_retention = calculate_information_retention(full_doc_text,summarized_doc_text )\n",
    "print(f\"Information Retention: {information_retention:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability\n",
    "Measure the average sentence length in the summary. Shorter sentences are often more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability Score: 63.49\n"
     ]
    }
   ],
   "source": [
    "def calculate_readability(summary):\n",
    "    return flesch_reading_ease(summary)\n",
    "\n",
    "readability_score = calculate_readability(summarized_doc_text)\n",
    "print(f\"Readability Score: {readability_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarity and Conciseness\n",
    "Ensure that the language used in the summary is clear and concise, avoiding jargon or overly complex terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clarity and Conciseness Score: 35.95\n"
     ]
    }
   ],
   "source": [
    "def calculate_clarity_and_conciseness(text):\n",
    "    readability_score = flesch_reading_ease(text)\n",
    "    return readability_score\n",
    "\n",
    "# Example usage:\n",
    "text = \"This is an example sentence for measuring clarity and conciseness.\"\n",
    "clarity_score = calculate_clarity_and_conciseness(text)\n",
    "print(f\"Clarity and Conciseness Score: {clarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance\n",
    "Assess whether the summary aligns with its intended purpose. For example, if the summary is meant to provide an overview, it should focus on key points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "def calculate_relevance(original_text, summary):\n",
    "    vectorizer = CountVectorizer().fit_transform([original_text, summary])\n",
    "    vectors = vectorizer.toarray()\n",
    "    relevance_score = cosine_similarity([vectors[0]], [vectors[1]])[0][0]\n",
    "    return relevance_score\n",
    "\n",
    "relevance_score = calculate_relevance(full_doc_text, summarized_doc_text)\n",
    "print(f\"Relevance Score: {relevance_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engagement\n",
    "If applicable, track user engagement with the summary, such as click-through rates or time spent reading. Higher engagement may indicate better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engagement Score: 0.17\n"
     ]
    }
   ],
   "source": [
    "def calculate_engagement(text):\n",
    "    sentiment_score = TextBlob(text).sentiment.polarity\n",
    "    return sentiment_score\n",
    "\n",
    "engagement_score = calculate_engagement(summarized_doc_text)\n",
    "print(f\"Engagement Score: {engagement_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence\n",
    "Evaluate the use of transition words and phrases to ensure that the summary flows smoothly between sentences and paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.35\n"
     ]
    }
   ],
   "source": [
    "def calculate_coherence_score(summarized_text, num_topics=2):\n",
    "    # Tokenize the summarized text into a list of sentences\n",
    "    sentences = [sentence.strip() for sentence in summarized_text.split('.') if sentence]\n",
    "\n",
    "    # Tokenize sentences into words\n",
    "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "    # Create a dictionary and a corpus\n",
    "    dictionary = Dictionary(tokenized_sentences)\n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_sentences]\n",
    "\n",
    "    # Build an LDA model on the corpus\n",
    "    lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "    # Calculate coherence score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_sentences, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "\n",
    "    return coherence_score\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "coherence_score = calculate_coherence_score(summarized_doc_text)\n",
    "print(f\"Coherence Score: {coherence_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
